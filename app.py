import os
from flask import Flask, render_template, request, jsonify
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
from dotenv import load_dotenv
import time
from typing import Dict, Any

# Lade Umgebungsvariablen
load_dotenv()

app = Flask(__name__)
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

def extract_content(url: str) -> (str, str, str, str):
    """Extrahiert den relevanten Textinhalt, Betreiber, Title und Meta-Description."""
    try:
        # Add https:// if not present
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
            
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        for element in soup(['script', 'style', 'img', 'nav', 'footer', 'header']):
            element.decompose()
        text = ' '.join(soup.stripped_strings)
        owner = extract_owner(soup)
        # SEO: Title und Meta-Description
        title = soup.title.string.strip() if soup.title and soup.title.string else ''
        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})
        meta_desc = meta_desc_tag['content'].strip() if meta_desc_tag and meta_desc_tag.get('content') else ''
        return text[:10000], owner, title, meta_desc
    except Exception as e:
        raise Exception(f"Fehler beim Laden der Webseite: {str(e)}")

def generate_summary(text: str, language: str = "en", owner: str = None, title: str = '', meta_desc: str = '') -> str:
    max_retries = 3
    retry_delay = 2

    # SEO-Abschnitte (jetzt ganz oben, mit Emoji und dreifacher √úberschrift f√ºr mehr Fettung)
    seo_section = ''
    if title:
        if language == 'en':
            seo_section += f"### üè∑Ô∏è **SEO Title**\n{title}\n"
        elif language == 'de':
            seo_section += f"### üè∑Ô∏è **SEO-Titel**\n{title}\n"
        elif language == 'pl':
            seo_section += f"### üè∑Ô∏è **Tytu≈Ç SEO**\n{title}\n"
    if meta_desc:
        if language == 'en':
            seo_section += f"### üìù **Meta Description**\n{meta_desc}\n"
        elif language == 'de':
            seo_section += f"### üìù **Meta-Beschreibung**\n{meta_desc}\n"
        elif language == 'pl':
            seo_section += f"### üìù **Opis meta**\n{meta_desc}\n"

    owner_section = ""
    if owner:
        if language == "en":
            owner_section = f"### üë§ **Website Owner**\n{owner}\n"
        elif language == "de":
            owner_section = f"### üë§ **Betreiber**\n{owner}\n"
        elif language == "pl":
            owner_section = f"### üë§ **W≈Ça≈õciciel strony**\n{owner}\n"

    prompts = {
        "en": {
            "system": "You are an expert in website analysis. Answer in English.",
            "user": f"""Analyze the following text and create a structured summary in Markdown format:\n\n{text}\n\nThe summary should include the following elements. Please use the following formatting:\n- Place the SEO title and meta description at the very top as their own sections, with bold headings and a fitting emoji.\n- All section headings should be bold and have a fitting emoji (e.g. ### üìù **Meta Description**).\n- Do NOT use emojis in the bullet points or in the text within the sections.\n- Start with a 'TL;DR' section (1-2 sentences) summarizing the page in a nutshell.\n- If possible, extract the website owner from the legal notice (impressum) or footer and add it as a 'Website Owner' section.\n\n{seo_section}### ‚ö°Ô∏è **TL;DR**\n(A very short summary in 1-2 sentences)\n\n### üìÑ **Summary**\n(A short summary in 3-5 sentences)\n\n### üìö **Main Topics**\n(Bullet points with the main topics)\n\n### #Ô∏è‚É£ **Tags**\n(Bullet points with 3-5 relevant keywords)\n\n### üóÇÔ∏è **Content Type**\n(Detected content type, e.g. blog, shop, portfolio)\n\n### üåê **Language**\n(Detected language of the page)\n{owner_section}\nFormat the answer as Markdown and reply in English."""
        },
        "de": {
            "system": "Du bist ein Experte f√ºr Webseiten-Analyse. Antworte auf Deutsch.",
            "user": f"""Analysiere den folgenden Text und erstelle eine strukturierte Zusammenfassung im Markdown-Format:\n\n{text}\n\nBitte verwende folgendes Format:\n- Platziere den SEO-Titel und die Meta-Beschreibung ganz oben als eigene Abschnitte, mit fetter √úberschrift und passendem Emoji.\n- Alle Abschnitts√ºberschriften sollen fett und mit Emoji sein (z.B. ### üìù **Meta-Beschreibung**).\n- Verwende KEINE Emojis in den Bullet-Points oder im Text innerhalb der Abschnitte.\n- Beginne mit einem 'TL;DR'-Abschnitt (1-2 S√§tze), der die Seite auf den Punkt bringt.\n- Falls m√∂glich, extrahiere den Betreiber aus dem Impressum oder Footer und gib ihn als eigenen Abschnitt 'Betreiber' aus.\n\n{seo_section}### ‚ö°Ô∏è **TL;DR**\n(Eine sehr kurze Zusammenfassung in 1-2 S√§tzen)\n\n### üìÑ **Zusammenfassung**\n(Eine kurze Zusammenfassung in 3-5 S√§tzen)\n\n### üìö **Hauptthemen**\n(Bullet-Points mit den wichtigsten Themen)\n\n### #Ô∏è‚É£ **Tags**\n(Bullet-Points mit 3-5 relevanten Keywords)\n\n### üóÇÔ∏è **Inhaltstyp**\n(Erkannter Inhaltstyp, z.B. Blog, Shop, Portfolio)\n\n### üåê **Sprache**\n(Erkannte Sprache der Seite)\n{owner_section}\nFormatiere die Antwort als Markdown und antworte auf Deutsch."""
        },
        "pl": {
            "system": "Jeste≈õ ekspertem od analizy stron internetowych. Odpowiadaj po polsku.",
            "user": f"""Przeanalizuj poni≈ºszy tekst i stw√≥rz uporzƒÖdkowane podsumowanie w formacie Markdown:\n\n{text}\n\nU≈ºyj nastƒôpujƒÖcego formatu:\n- Umie≈õƒá tytu≈Ç SEO i meta opis na samej g√≥rze jako osobne sekcje, z pogrubionym nag≈Ç√≥wkiem i emoji.\n- Wszystkie nag≈Ç√≥wki sekcji powinny byƒá pogrubione i mieƒá emoji (np. ### üìù **Opis meta**).\n- NIE u≈ºywaj emoji w punktach listy ani w tre≈õci sekcji.\n- Zacznij od sekcji 'TL;DR' (1-2 zdania), kt√≥ra podsumowuje stronƒô w skr√≥cie.\n- Je≈õli to mo≈ºliwe, wyodrƒôbnij w≈Ça≈õciciela strony z impressum lub stopki i dodaj sekcjƒô 'W≈Ça≈õciciel strony'.\n\n{seo_section}### ‚ö°Ô∏è **TL;DR**\n(Bardzo kr√≥tkie podsumowanie w 1-2 zdaniach)\n\n### üìÑ **Podsumowanie**\n(Kr√≥tkie podsumowanie w 3-5 zdaniach)\n\n### üìö **G≈Ç√≥wne tematy**\n(Punktory z g≈Ç√≥wnymi tematami)\n\n### #Ô∏è‚É£ **Tagi**\n(Punktory z 3-5 odpowiednimi s≈Çowami kluczowymi)\n\n### üóÇÔ∏è **Typ tre≈õci**\n(Rozpoznany typ tre≈õci, np. blog, sklep, portfolio)\n\n### üåê **Jƒôzyk**\n(Rozpoznany jƒôzyk strony)\n{owner_section}\nSformatuj odpowied≈∫ jako Markdown i odpowiedz po polsku."""
        }
    }
    prompt = prompts.get(language, prompts["en"])

    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": prompt["system"]},
                    {"role": "user", "content": prompt["user"]}
                ],
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            if attempt == max_retries - 1:
                raise Exception(f"API error after {max_retries} attempts: {str(e)}")
            time.sleep(retry_delay * (attempt + 1))

def extract_owner(soup) -> str:
    # Suche nach Impressum/Legal Notice oder Footer
    # 1. Suche nach Linktexten
    impressum_links = soup.find_all('a', string=lambda s: s and ('impressum' in s.lower() or 'legal' in s.lower() or 'contact' in s.lower()))
    for link in impressum_links:
        href = link.get('href', '')
        if href and not href.startswith('#'):
            try:
                if href.startswith('http'):
                    resp = requests.get(href, timeout=5)
                else:
                    base = soup.find('base')
                    base_url = base['href'] if base and 'href' in base.attrs else ''
                    resp = requests.get(base_url + href, timeout=5)
                sub_soup = BeautifulSoup(resp.text, 'html.parser')
                owner = extract_owner_from_text(sub_soup.get_text(" ", strip=True))
                if owner:
                    return owner
            except Exception:
                continue
    # 2. Suche im Footer
    footer = soup.find('footer')
    if footer:
        owner = extract_owner_from_text(footer.get_text(" ", strip=True))
        if owner:
            return owner
    # 3. Suche im gesamten Text
    owner = extract_owner_from_text(soup.get_text(" ", strip=True))
    return owner

def extract_owner_from_text(text: str) -> str:
    # Einfache Heuristik f√ºr typische Betreiberangaben
    import re
    patterns = [
        r"(?:Inhaber|Betreiber|Owner|Herausgeber|Verantwortlich(?:er)?|Impressum)[:\s]*([A-Z√Ñ√ñ√ú][\w\s,&.-]{3,100})",
        r"([A-Z][a-z]+ [A-Z][a-z]+)\s*(GmbH|AG|UG|Inc\.|LLC|Ltd\.|S\.A\.|S\.r\.l\.|Sp\.z o\.o\.|e\.K\.|KG|OHG)?"
    ]
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(0)
    return None

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/summarize', methods=['POST'])
def summarize():
    try:
        url = request.form.get('url')
        language = request.form.get('language', 'en')
        if not url:
            return jsonify({'error': 'URL ist erforderlich'}), 400
        content, owner, title, meta_desc = extract_content(url)
        summary = generate_summary(content, language, owner, title, meta_desc)
        if request.form.get('save') == 'true':
            with open('summary.md', 'w', encoding='utf-8') as f:
                f.write(summary)
        return jsonify({'summary': summary})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    import os
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port) 
